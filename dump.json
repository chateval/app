[{"model": "orm.metric", "pk": 1, "fields": {"name": "Average Sentence Length", "info": "The average number of tokens in the model's responses."}}, {"model": "orm.metric", "pk": 2, "fields": {"name": "Distinct 1", "info": "The number of unique unigrams in the model's responses divided by the total number of generated tokens."}}, {"model": "orm.metric", "pk": 3, "fields": {"name": "Distinct 2", "info": "The number of unique bigrams in the model's responses divided by the total number of generated tokens."}}, {"model": "orm.metric", "pk": 4, "fields": {"name": "Embedding Greedy Match Score", "info": "Greedy matching between word embeddings of target utterance and model utterance (Rus et al., 2012)."}}, {"model": "orm.metric", "pk": 5, "fields": {"name": "Embedding Extrema Score", "info": "Vector extrema of a model's response token vectors (Forgues et al., 2014)."}}, {"model": "orm.metric", "pk": 6, "fields": {"name": "Average Embedding Score", "info": "Average of a model's responses token vectors."}}, {"model": "orm.metric", "pk": 7, "fields": {"name": "BLEU Score", "info": "The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence."}}]