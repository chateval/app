{% extends 'base.html' %}

{% block content %}
<section class="hero has-text-centered is-success is-medium">
  <div class="hero-body">
    <div class="container" style="text-align:left">
      <h1 class="title">
        ChatEval
      </h1>
      <p> ChatEval is a scientific framework for evaluating neural open domain chatbots. Researchers can submit their trained models to effortlessly receive comparisons with baselines and prior work. Since all evaluation code is open-source, we ensure evaluation is performed in a standardized and transparent way. Additionally, open source baseline models and an ever growing groups public evaluation sets are available for public use.</p>
      <br>
      <a class="button is-light is-outlined is-large" href="/submit"> Upload Model</a>
    </div>
  </div>
</section>

<div class="container is-fluid">
  <div class="columns">
    <div class="column is-one-sixth"></div>
    <div class="column is-two-thirds">
        <br> <br>
        <h1 class="title"> Evaluation Datasets </h1>
        <h4 style="margin-bottom: 2.5rem">
          ChatEval allows you to directly compare your model's responses to other models' responses on a standardized set of prompts. When you upload your model, you can choose to include responses to the prompts of any of the following evaluation datasets. While we currently only support datasets with single-turn prompts, stay-tuned for multi-turn prompt datasets coming soon. If you would like to propose a new evaluation dataset, submit a pull request <a href="https://github.com/chateval/ChatEval/tree/master/eval_data">here</a>. 
        </h4>
        <div class="columns is-multiline" style="padding-bottom:0rem; padding-top: 0rem">  
          {% for dataset in datasets %}
          <div class="column is-half" style="padding-top: 0rem; padding-bottom: 0rem;">
            <div class="box">
              <article class="media">
                <div class="media-content">
                  <div class="content">
                    <p>
                      <strong> {{dataset.long_name}} </strong> <br>
                      <small> {{dataset.description}} </small>
                      <br/>
                      <a class="button is-outlined is-small" href="{{dataset.source}}"> Download </a>
                      <br>
                    </p>
                  </div>
                </div>
              </article>
            </div>
          </div>
          <br>
          {% endfor %}    
        </div>

        <h1 class="title"> ChatEval Baselines </h1>
        <h4 style="margin-bottom: 2.5rem">
          In the ChatEval sytem, you can choose to have your model's responses compared to those of other public submitted models or to reference human responses. In addition to providing reference human response sets for all of the above datasets, the ChatEval system also includes responses generated from standardized open-source Seq2Seq baselines. These are described in further detail below.
        </h4>
        <div class="columns is-multiline" style="padding-bottom:0rem; padding-top: 0rem">             
          {% for baseline in baselines %}
          <div class="column is-half" style="padding-top: 0rem; padding-bottom: 0rem;">
            <div class="box">
              <article class="media">
                <div class="media-content">
                  <div class="content">
                    <p>
                      <strong> {{baseline.model.name}} </strong> <br>
                      <small> {{baseline.model.description}} </small>
                      <br/>
                      <a class="button is-outlined is-small" href="/model?model_id={{ baseline.model.model_id }}"> View </a>
                      <br>
                    </p>
                  </div>
                </div>
              </article>
            </div>
          </div>
          {% endfor %}
        </div>

        <h1 class="title"> Automated Evaluation Methods </h1>
        <h4  style="margin-bottom: 2.5rem">We currently support the following evaluation metrics. These are computed automatically for every model uploaded to the ChatEval system. Implementations can be found <a href='https://github.com/chateval/evaluation/blob/master/auto_eval_utils.py'>here</a>. If you would like to propose an additional evaluation metric, please submit a pull request.
        </h4>
        <div class="columns is-multiline" style="padding-bottom:0rem; padding-top: 0rem">   
          
          {% for metric in metrics %}
          <div class="column is-half" style="padding-top: 0rem; padding-bottom: 0rem;">
            <div class="box" id="{{ metric.metric_id }}">
              <article class="media">
                <div class="media-content">
                  <div class="content">
                    <p>
                      <strong> {{ metric.name }} </strong> <br>
                      <small> {{ metric.info }} </small>
                      <br>
                    </p>
                  </div>
                </div>
              </article>
            </div>
          </div>
          {% endfor %}
        </div>

        <h1 class="title" style="margin-bottom: 2.5rem"> References </h1>
        <div class="columns is-multiline" style="padding-bottom:0rem; padding-top: 0rem">
          <p>
            Higashinaka, Ryuichiro, Kotaro Funakoshi, Yuka Kobayashi, and Michimasa Inaba. "The dialogue breakdown detection challenge: Task description, datasets, and evaluation metrics." In <i>LREC</i>. 2016.
          </p>
          <br/>
          <p>
            Liu, Chia-Wei, Ryan Lowe, Iulian Serban, Mike Noseworthy,Laurent Charlin, and Joelle Pineau. "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation." In <i>EMNLP</i>, pp. 2122–2132. Association for Computational Linguistics,  2016.
          </p>
          <br/>
          <p>
            Forgues, Gabriel, Joelle Pineau, Jean-Marie Larchevêque, and Réal Tremblay. "Bootstrapping dialog systems with word embeddings." In <i>NIPS, modern machine learning and natural language processing workshop</i>, vol. 2. 2014.
          </p>
          <br/>
          <p>
            Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. "BLEU: a method for automatic evaluation of machine translation." In <i>Proceedings of the 40th annual meeting on association for computational linguistics</i>, pp. 311-318. Association for Computational Linguistics, 2002.
          </p>
          <br/>
          <p>
            Rus, Vasile, and Mihai Lintean. "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics." In <i>Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</i>, pp. 157-162. Association for Computational Linguistics, 2012.
          </p>
          <br/>
          <p>
            Tiedemann, Jörg. "News from OPUS-A collection of multilingual parallel corpora with tools and interfaces." In Recent advances in natural language processing, vol. 5, pp. 237-248. 2009.
          </p>
          <br/>
          <p>
            Vinyals, Oriol, and Quoc Le. "A neural conversational model." arXiv preprint arXiv:1506.05869 (2015).
          </p>
          <br/>
        </div>
      </div>
    <div class="column is-one-sixth"></div>
  </div>
  <br>
</div>
{% endblock %}
